{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Regression\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab you will apply regression to some realistic data. In this lab you will work with the automotive price dataset. Your goal is to construct a linear regression model to predict the price of automobiles from their characteristics. \n",
    "\n",
    "In this lab will learn to:\n",
    "\n",
    "1. Use categorical data with scikit-learn. \n",
    "2. Apply transformations to features and labels to improve model performance. \n",
    "3. Compare regression models to improve model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "As a first, step you will load the dataset into the notebook environment. \n",
    "\n",
    "First, execute the code in the cell below to load  the packages you will need to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below loads the dataset which was prepared using steps from the Data Preparation lab.Execute this code and ensure that the expected columns are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices = pd.read_csv('Auto_Data_Preped.csv')\n",
    "auto_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are both numeric and categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model matrix.\n",
    "\n",
    "All scikit-learn models require a numpy array of numeric only values for the features. The resulting array is often referred to as the **model matrix**. \n",
    "\n",
    "To create a model matrix from cases with both numeric and categorical variables requires two steps. First, the numeric features must be rescaled. Second, the categorical variables must be converted to a set of **dummy variables** to encode the presence or not of each category.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variables from categorical features\n",
    "\n",
    "Now, you must create dummy variables for the categorical features. Dummy variables encode categorical features as a set of binary variables. There is one dummy variable for each possible category. For each case all of the values in the dummy variables are set to zero, except the one corresponding to the category value, which is set to one. In this way, a categorical variable with any number of categories can be encoded as series of numeric features which scikit-learn can operate on. This process is referred to as **one hot encoding** since only one dummy variable is coded as 1 (hot) per case. \n",
    "\n",
    "The `sklearn.preprocessing` package contains functions to encode categorical features as dummy variables in two steps;\n",
    "1. The categories are  encoded as numbers starting with 0. For example, if there are 5 categories, they are encoded as the set $\\{ 0,1,2,3,4 \\}$.\n",
    "2. The numeric categories are then encoded as dummy variables. \n",
    "\n",
    "The following example will give you a feel for how this process works. The code in the cell below computes the numeric representation of the categories for the `body_style` feature by the following steps:\n",
    "\n",
    "1. An encoder object is created using the `LabelEncoder` method.\n",
    "2. The encoder is `fit` to the unique string values of the feature. \n",
    "3. The `transformation` method then applies the numeric encoding to the original feature. \n",
    "\n",
    "Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(auto_prices['body_style'].unique())\n",
    "Features = auto_prices['body_style']\n",
    "enc = preprocessing.LabelEncoder()\n",
    "enc.fit(Features)\n",
    "Features = enc.transform(Features)\n",
    "print(Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this five original body style categories of this feature is now coded as integers in the set $\\{ 0,1,2,3,4 \\}$.\n",
    "\n",
    "For the next step in the process, the numerically coded categorical variable is converted to a set of dummy variables following these steps:\n",
    "1. A one hot encoder object is created using the `OneHotEncoder` method from the `sklearn.preprocessing` module.\n",
    "2. The numerically coded categorical feature is fit with the one hot encoder. \n",
    "3. The dummy variables are encoded using the `transform` method on the encodings.\n",
    "\n",
    "Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ohe = preprocessing.OneHotEncoder()\n",
    "encoded = ohe.fit(Features.reshape(-1,1))\n",
    "Features = encoded.transform(Features.reshape(-1,1)).toarray()\n",
    "Features[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `body_style` feature has been encoded as five columns. Each of these columns is a dummy variable representing one category. Each row has one and only one dummy variable with a 1, and the rest 0s. This is the one hot encoding. \n",
    "\n",
    "Now, you need to one hot encode all five categorical variables and append them as columns to the model matrix with the scaled numeric variables. The code in the cell below executes a `for` loop that calls the `encode_string` function and uses the numpy `concatenate` function to add the dummy variables to the model matrix. The `encode_string` function uses the same process discussed above. \n",
    "\n",
    "Execute this code, verify the result, and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_string(cat_feature):\n",
    "    ## First encode the strings to numeric categories\n",
    "    enc = preprocessing.LabelEncoder()\n",
    "    enc.fit(cat_feature)\n",
    "    enc_cat_feature = enc.transform(cat_feature)\n",
    "    ## Now, apply one hot encoding\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n",
    "    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n",
    "    \n",
    "\n",
    "categorical_columns = ['fuel_type', 'aspiration', 'drive_wheels', 'num_of_cylinders']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    temp = encode_string(auto_prices[col])\n",
    "    Features = np.concatenate([Features, temp], axis = 1)\n",
    "\n",
    "print(Features.shape)\n",
    "print(Features[:2, :])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model matrix now has 14 features which encode the five origianalcategorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the numeric features\n",
    "\n",
    "To complete the model matrix, execute the code in the cell below to concatenate the three numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.concatenate([Features, np.array(auto_prices[['curb_weight', 'horsepower', 'city_mpg']])], axis = 1)\n",
    "Features[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 17 features, 14 dummy variables and 3 numeric features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "\n",
    "With the model matrix constructed, you must now create randomly sampled training and test data sets. The code in the cell below uses the `train_test_split` function from the `sklearn.model_selection` module to Bernoulli sample the cases in the original dataset into the two subsets. Since this data set is small only 40 cases will be included in the test dataset. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(9988)\n",
    "labels = np.array(auto_prices['log_price'])\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 40)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale numeric features\n",
    "\n",
    "Numeric features must be rescaled so they have a similar range of values. Rescaling prevents features from having an undue influence on model training simply because then have a larger range of numeric variables. \n",
    "\n",
    "The code in the cell below uses the `StandardScaler` function from the Scikit Learn preprocessing package to Zscore scale the numeric features. Notice that the scaler is fit only on the training data. The trained scaler is these applied to the test data. Test data should always be scaled using the parameters from the training data. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x_train[:,14:])\n",
    "x_train[:,14:] = scaler.transform(x_train[:,14:])\n",
    "x_test[:,14:] = scaler.transform(x_test[:,14:])\n",
    "print(x_train.shape)\n",
    "x_train[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the numeric features have been rescaled are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the linear regression model\n",
    "\n",
    "With data prepared and split into training and test subsets, you will now compute the linear regression model. With the dummy variables created there are 17 features, so the model will require 17 coefficients. There is no intercept specified since we are working with dummy variables. The equation for such a **multiple regression** problem can be written as:\n",
    "\n",
    "$$\\hat{y} = f(\\vec{x}) = \\vec{\\beta} \\cdot \\vec{x} + b\\\\ = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + b$$  \n",
    "where; \n",
    "$\\hat{y}$ are the predicted values or scores,   \n",
    "$\\vec{x}$ is the vector of feature values with components $\\{ x_1, x_2, \\cdots, x_n$,  \n",
    "$\\vec{\\beta}$ is vector of model coefficients with components $\\{ \\beta_1, \\beta_2, \\cdots, \\beta_n$,  \n",
    "$b$ is the intercept term, if there is one.\n",
    "\n",
    "You can think of the linear regression function $f(\\vec{x})$ as the dot product between the beta vector $\\vec{\\beta}$ and the feature vector $\\vec{x}$, plus the intercept term $b$.\n",
    "\n",
    "The code in the cell below uses the `sklearn import linear_model` to compute a least squares linear model as follows:\n",
    "1. A linear regression model object is created with the `LinearRegression` method. Notice, that in this case, no intercept will be fit. The intercept value or **bias** will be accommodated in the coefficients of the dummy variables for the categorical features. \n",
    "2. The model is fit using the `fit` method with the numpy array of features and the label. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define and fit the linear regression model\n",
    "lin_mod = linear_model.LinearRegression(fit_intercept = False)\n",
    "lin_mod.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been fit to the training data. Execute the code in the cell below to examine the value of the intercept term and coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin_mod.intercept_)\n",
    "print(lin_mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the intercept term is `0.0`. Roughly speaking, you can interpret the coefficients of the model as follows:  \n",
    "1. The price of autos increases with weight (first coefficient), horsepower (second coefficient) and weakly decreases with fuel efficiency (third coefficient). \n",
    "2. The coefficients for the dummy variables are in a similar range, indicating the bias or intercept has been incorporated in these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "You will now use the test dataset to evaluate the performance of the regression model. As a first step, execute the code in the cell below to compute and display various performance metrics and examine the results. Then, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_predicted, n_parameters):\n",
    "    ## First compute R^2 and the adjusted R^2\n",
    "    r2 = sklm.r2_score(y_true, y_predicted)\n",
    "    r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n",
    "    \n",
    "    ## Print the usual metrics and the R^2 values\n",
    "    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
    "    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
    "    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
    "    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n",
    "    print('R^2                    = ' + str(r2))\n",
    "    print('Adjusted R^2           = ' + str(r2_adj))\n",
    "   \n",
    "y_score = lin_mod.predict(x_test) \n",
    "print_metrics(y_test, y_score, 28)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, these metrics look promising. The RMSE, MAE and median absolute error are all small and in a similar range. However, notice that the $R^2$ and $R^2_{adj}$ are rather different. This model has a large number of parameters compared to the number of cases available. This result indicates that the model may be overfit and might not generalize well. \n",
    "\n",
    "To continue the evaluation of the model performance, execute the code in the cell below to display a histogram of the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_resids(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    sns.distplot(resids)\n",
    "    plt.title('Histogram of residuals')\n",
    "    plt.xlabel('Residual value')\n",
    "    plt.ylabel('count')\n",
    "    \n",
    "hist_resids(y_test, y_score)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows that the residuals are in a small range. However, there is some noticeable skew in the distribution. \n",
    "\n",
    "Next, execute the code in the cell below to display the Q-Q Normal plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_qq(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    ss.probplot(resids.flatten(), plot = plt)\n",
    "    plt.title('Residuals vs. predicted values')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residual')\n",
    "    \n",
    "resid_qq(y_test, y_score)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the histogram, the Q-Q Normal plot indicates the residuals are close to Normally distributed, show some skew (deviation from the straight line). This is particularly for large residuals. \n",
    "\n",
    "There is one more diagnostic plot. Execute the code in the cell below to display the plot of residuals vs. predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_plot(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    sns.regplot(y_score, resids, fit_reg=False)\n",
    "    plt.title('Residuals vs. predicted values')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residual')\n",
    "\n",
    "resid_plot(y_test, y_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks reasonable. The residual values appear to have a fairly constant dispersion as the predicted value changes. A few large residuals are noticeable, particularly on the positive side.\n",
    "\n",
    "But, wait! This residual plot is for the log of the auto price. What does the plot look like when transformed to real prices? Execute the code in the cell below to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_untransform = np.exp(y_score)\n",
    "y_test_untransform = np.exp(y_test)\n",
    "resid_plot(y_test_untransform, y_score_untransform) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the untransformed residuals show a definite trend. The dispersion of the residuals has a cone-like pattern increasing to the right. The regression model seems to do a good job of predicting the price of low cost cars, but becomes progressively worse as the price of the car increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson you have done the following in the process of constructing and evaluating a multiple linear regression model:   \n",
    "1. Transformed the label value to make it more symmetric and closer to a Normal distribution.\n",
    "2. Aggregated categories of a categorical variable to improve the statistical representation. \n",
    "3. Scaled the numeric features. \n",
    "4. Recoded the categorical features as binary dummy variables. \n",
    "5. Fit the linear regression model using scikit-learn. \n",
    "6. Evaluated the performance of the model using both numeric and graphical methods. \n",
    "\n",
    "It is clear from the outcome of the performance evaluation that this model needs to be improved. As it is, the model shows poor generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
