{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with principle components\n",
    "\n",
    "**Principle component analysis**, or **PCA**, is an alternative to regularization and straight-forward feature elimination. PCA is particularly useful for problems with very large numbers of features compared to the number of training cases. For example, when faced with a problem with many thousands of features and perhaps a few thousand cases, PCA can be a good choice to **reduce the dimensionality** of the feature space.  \n",
    "\n",
    "PCA is one of a family of transformation methods that reduce dimensionality. PCA is the focus here, since it is the most widely used of these methods. \n",
    "\n",
    "The basic idea of PCA is rather simple: Find a linear transformation of the feature space which **projects the majority of the variance** onto a few orthogonal dimensions in the transformed space. The PCA transformation maps the data values to a new coordinate system defined by the principle components. Assuming the highest variance directions, or **components**, are the most informative, low variance components can be eliminated from the space with little loss of information. \n",
    "\n",
    "The projection along which the greatest variance occurs is called the **first principle component**. The next projection, orthogonal to the first, with the greatest variance is call the **second principle component**. Subsequent components are all mutually orthogonal with decreasing variance along the projected direction.  \n",
    "\n",
    "Widely used PCA algorithms compute the components sequentially, starting with the first principle component. This means that it is computationally efficient to compute the first several components from a very large number of features. Thus, PCA can make problems with very large numbers of features computationally tractable. \n",
    "\n",
    "****\n",
    "**Note:** It may help your understanding to realize that principle components are a scaled version of the **eigenvectors** of the feature matrix. The scale for each dimensions is given by the **eigenvalues**. The eigenvalues are the fraction of the variance explained by the components. \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "To cement the concepts of PCA you will now work through a simple example. This example is restricted to 2-d data so that the results are easy to visualize. \n",
    "\n",
    "As a first step, execute the code in cell below to load the packages required for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "import sklearn.decomposition as skde\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below simulates data from a bivariate Normal distribution. The distribution is deliberately centered on $\\{ 0,0 \\}$ and with unit variance on each dimension. There is considerable correlation between the two dimensions leading to a covariance matrix:\n",
    "\n",
    "$$cov(X) =  \\begin{bmatrix}\n",
    "  1.0 & 0.6 \\\\\n",
    "  0.6 & 1.0\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "Given the covariance matrix 100 draws from this distribution are computed using the `multivariate_normal` function from the Numpy `random` package. Execute this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(124)\n",
    "cov = np.array([[1.0, 0.6], [0.6, 1.0]])\n",
    "mean = np.array([0.0, 0.0])\n",
    "\n",
    "sample = nr.multivariate_normal(mean, cov, 100)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for this data, execute the code in the cell below to display a plot and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data have a roughly elliptical pattern. The correlation between the two dimensions is also visible. \n",
    "\n",
    "With the simulated data set created, it is time to compute the PCA model. The code in the cell below does the following:\n",
    "1. Define a PCA model object using the `PCA` function from the scikit-learn `decomposition` package.\n",
    "2. Fit the PCA model to the sample data.\n",
    "3. Display the ratio of the **variance explained** by each of the components, where, for a matrix X, this ratio is given by:\n",
    "\n",
    "$$VE(X) = \\frac{Var_{X-component}(X)}{Var_{X-total}(X)}$$\n",
    "\n",
    "Notice that by construction:\n",
    "\n",
    "$$VE(X) = \\sum_{i=1}^N VE_i(X) = 1.0$$\n",
    "\n",
    "In other words, the sum of the variance explained for each component must add to the total variance or 1.0 for standardized data. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = skde.PCA()\n",
    "pca_fit = pca_model.fit(sample)\n",
    "print(pca_fit.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the explained variance of the first component is many times larger than for the second component. This is exactly the desired result indicating the first principle component explains the majority of the variance of the sample data. \n",
    "\n",
    "The code in the cell below computes and prints the scaled components. Mathematically, the scaled components are the eigenvectors scaled by the eigenvalues. Execute this code:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = pca_fit.components_\n",
    "for i in range(2):\n",
    "    comps[:,i] = comps[:,i] * pca_fit.explained_variance_ratio_\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two vectors have their origins at $\\[ 0,0 \\}$, and are quite different magnitude, and are pointing in different directions.  \n",
    "\n",
    "To better understand how the projections of the components relate to the data, execute the code to plot the data along with the principle components. Execute this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "# plt.plot([0.0, comps[0,0]], [0.0,comps[0,1]], color = 'red', linewidth = 5)\n",
    "# plt.plot([0.0, comps[1,0]], [0.0,comps[1,1]], color = 'red', linewidth = 5)\n",
    "plt.plot([0.0, comps[0,0]], [0.0,comps[0,1]], color = 'red', linewidth = 5)\n",
    "plt.plot([0.0, comps[1,0]], [0.0,comps[1,1]], color = 'red', linewidth = 5)\n",
    "\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the fist principle component (the long red line) is along the direction of greatest variance of the data. This is as expected. The short red line is along the direction of the second principle component. The lengths of these lines is the variance in the directions of the projection. \n",
    "\n",
    "The ultimate goal of PCA is to transform data to a coordinate system with the highest variance directions along the axes. The code in the cell below uses the `transform` method on the PCA object to perform this operation and then plots the result. Execute this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pca_fit.transform(sample)\n",
    "plt.scatter(trans[:,0], trans[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the scale along these two coordinates are quite different. The first principle component is along the horizontal axis. The range of values on this direction is in the range of about $\\{ -2.5,2.5 \\}$. The range of values on the vertical axis or second principle component are only about $\\{ -0.2, 0.3 \\}$. It is clear that most of the variance is along the direction of the fist principle component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features and Labels\n",
    "\n",
    "Keeping the foregoing simple example in mind, it is time to apply PCA to some real data. \n",
    "\n",
    "The code in the cell below loads the dataset which has had the the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "3. Encoding categorical variables as binary dummy variables.\n",
    "4. Standardizing numeric variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 features in this data set. The numeric features have been Zscore scaled so they are zero centered (mean removed) and unit variance (divide by standard deviation). \n",
    "\n",
    "****\n",
    "**Note:** Before performing PCA all features must be zero mean and unit variance. Failure to do so will result in biased computation of the components and scales. In this case, the data set has already been scaled, but ordinarily scaling is a key step. \n",
    "****\n",
    "\n",
    "Now, run the code in the cell below to split the data set into test and training subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute principle components\n",
    "\n",
    "The code in the cell below computes the principle components for the training feature subset. Execute this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mod = skde.PCA()\n",
    "pca_comps = pca_mod.fit(X_train)\n",
    "pca_comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to print the variance explained for each component and the sum of the variance explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_comps.explained_variance_ratio_)\n",
    "print(np.sum(pca_comps.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are a bit abstract. However, you can see that the variance ratios are in descending order and that the sum is 1.0. \n",
    "\n",
    "Execute the code in the cell below to create a plot of the explained variance vs. the component:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained(mod):\n",
    "    comps = mod.explained_variance_ratio_\n",
    "    x = range(len(comps))\n",
    "    x = [y + 1 for y in x]          \n",
    "    plt.plot(x,comps)\n",
    "\n",
    "plot_explained(pca_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve is often referred to as a **scree plot**. Notice that the explained variance decreases rapidly until the 5th component and then slowly, thereafter. The first few components explain a large fraction of the variance and therefore contain much of the explanatory information in the data. The components with small explained variance are unlikely to contain much explanatory information. Often the inflection point or 'knee' in the scree curve is used to choose the number of components selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create a PCA model with a reduced number of components. The code in the cell below trains and fits a PCA model with 5 components, and then transforms the features using that model. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mod_5 = skde.PCA(n_components = 5)\n",
    "pca_mod_5.fit(X_train)\n",
    "Comps = pca_mod_5.transform(X_train)\n",
    "Comps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and evaluate a logistic regression model\n",
    "\n",
    "Next, you will compute and evaluate a logistic regression model using the features transformed by the first 5 principle components. Execute the code in the cell below to define and fit a logistic regression model, and print the model coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define and fit the logistic regression model\n",
    "log_mod_5 = linear_model.LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) \n",
    "log_mod_5.fit(Comps, y_train)\n",
    "print(log_mod_5.intercept_)\n",
    "print(log_mod_5.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now 5 regression coefficients, one for each component. This number is in contrast to the 35 features in the raw data. \n",
    "\n",
    "Next, evaluate this model using the code below. Notice that the test features are transformed using the same PCA transformation used for the training data. Execute this code and examine the results.\n",
    "\n",
    "Then, answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "\n",
    "def plot_auc(labels, probs):\n",
    "    ## Compute the false positive rate, true positive rate\n",
    "    ## and threshold along with the AUC\n",
    "    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n",
    "    auc = sklm.auc(fpr, tpr)\n",
    "    \n",
    "    ## Plot the result\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()    \n",
    "\n",
    "probabilities = log_mod_5.predict_proba(pca_mod_5.transform(X_test))\n",
    "print_metrics(y_test, probabilities, 0.3)    \n",
    "plot_auc(y_test, probabilities)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, these results look good. The question remains, were the correct number of principle components used? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more components to the model\n",
    "\n",
    "Now you will compute and evaluate a logistic regression model using the first 10 principle components. You will compare this model to the one created with 5 principle components. Execute the code below to transform the training features using the first 10 principle components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mod_10 = skde.PCA(n_components = 10)\n",
    "pca_mod_10.fit(X_train)\n",
    "Comps_10 = pca_mod_10.transform(X_train)\n",
    "Comps_10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to define and fit a logistic regression model using the 10 components of the transformed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define and fit the linear regression model\n",
    "log_mod_10 = linear_model.LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) \n",
    "log_mod_10.fit(Comps_10, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below scores the logistic regression model and displays performance metrics, the ROC curve, and the AUC. Execute this code and examine the result. \n",
    "\n",
    "Then, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = log_mod_10.predict_proba(pca_mod_10.transform(X_test))\n",
    "print_metrics(y_test, probabilities, 0.3)  \n",
    "plot_auc(y_test, probabilities)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the metrics have improved compared to the 5 component model. Apparently there is useful information in the first 10 components. \n",
    "\n",
    "But, is this difference really significant. To find out, you will now perform cross validation on the result. Ideally, the fitting of the PCA model should be part of the cross validation process. However, at the risk of a small bias, this step is omitted for the sake of simplicity. Execute the code in the cell below to perform the cross validation and display the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_format(f,x,y,z):\n",
    "    print('Fold %2d    %4.3f        %4.3f      %4.3f' % (f, x, y, z))\n",
    "\n",
    "def print_cv(scores):\n",
    "    fold = [x + 1 for x in range(len(scores['test_precision_macro']))]\n",
    "    print('         Precision     Recall       AUC')\n",
    "    [print_format(f,x,y,z) for f,x,y,z in zip(fold, scores['test_precision_macro'], \n",
    "                                          scores['test_recall_macro'],\n",
    "                                          scores['test_roc_auc'])]\n",
    "    print('-' * 40)\n",
    "    print('Mean       %4.3f        %4.3f      %4.3f' % \n",
    "          (np.mean(scores['test_precision_macro']), np.mean(scores['test_recall_macro']), np.mean(scores['test_roc_auc'])))  \n",
    "    print('Std        %4.3f        %4.3f      %4.3f' % \n",
    "          (np.std(scores['test_precision_macro']), np.std(scores['test_recall_macro']), np.std(scores['test_roc_auc'])))\n",
    "  \n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "scoring = ['precision_macro', 'recall_macro', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mod = skde.PCA(n_components = 5)\n",
    "pca_mod.fit(Features)\n",
    "Comps = pca_mod.transform(Features)\n",
    "\n",
    "scores = ms.cross_validate(log_mod_5, Comps, Labels, scoring=scoring,\n",
    "                        cv=10, return_train_score=False)\n",
    "print_cv(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mod = skde.PCA(n_components = 10)\n",
    "pca_mod.fit(Features)\n",
    "Comps = pca_mod.transform(Features)\n",
    "\n",
    "scores = ms.cross_validate(log_mod_10, Comps, Labels, scoring=scoring,\n",
    "                        cv=10, return_train_score=False)\n",
    "print_cv(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the AUC and its standard deviation obtained above to the AUC of the 5 component model. The difference does appear to be significant. This difference supports the hypothesis that the first 10 components all contain useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have applied principle component analysis to dimensionality reduction for supervised machine learning. The first components computed contain most of the available information. When faced with large number of features, PCA is an effective way to make supervised machine learning models tractable. \n",
    "\n",
    "Specifically in this lab you have:\n",
    "1. Computed PCA models with different numbers of components.\n",
    "2. Compared logistic regression models with different numbers of components. In this case, using 10 components produced a significantly better model. Using 10 components is a useful reduction in dimensionality compared to the original 35 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
