{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks\n",
    "\n",
    "**Neural networks** are a powerful set of machine learning algorithms. Neural network use one or more **hidden layers** of multiple **hidden units** to perform **function approximation**. The use of multiple hidden units in one or more layers, allows neural networks to approximate complex functions. Neural network models capable of approximating complex functions are said to have high **model capacity**. This property allows neural networks to solve complex machine learning problems. \n",
    "\n",
    "However, because of the large number of hidden units, neural networks have many **weights** or **parameters**. This situation often leads to **over-fitting** of neural network models, which limits generalization. Thus, finding optimal hyperparameters when fitting neural network models is essential for good performance. \n",
    "\n",
    "An additional issue with neural networks is **computational complexity**. Many optimization iterations are required. Each optimization iteration requires the update of a large number of parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use neutral network models to classify the species of iris flowers using the famous iris dataset. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for these data, you will now load and plot them. The code in the cell below does the following:\n",
    "\n",
    "1. Loads the iris data as a Pandas data frame. \n",
    "2. Adds column names to the data frame.\n",
    "3. Displays all 4 possible scatter plot views of the data. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris(iris):\n",
    "    '''Function to plot iris data by type'''\n",
    "    setosa = iris[iris['Species'] == 'setosa']\n",
    "    versicolor = iris[iris['Species'] == 'versicolor']\n",
    "    virginica = iris[iris['Species'] == 'virginica']\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = 'x')\n",
    "            ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = 'o')\n",
    "            ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = '+')\n",
    "            ax[i,j].set_xlabel(x_ax[i])\n",
    "            ax[i,j].set_ylabel(y_ax[j])\n",
    "            \n",
    "## Import the dataset from sklearn.datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "## Create a data frame from the dictionary\n",
    "species = [iris.target_names[x] for x in iris.target]\n",
    "iris = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\n",
    "iris['Species'] = species\n",
    "\n",
    "## Plot views of the iris data            \n",
    "plot_iris(iris) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will seperate these categories. \n",
    "\n",
    "Scikit Learn classifiers require numerically coded numpy arrays for the features and as a label. The code in the cell below does the following processing:\n",
    "1. Creates a numpy array of the features.\n",
    "2. Numerically codes the label using a dictionary lookup, and converts it to a numpy array. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(iris[['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']])\n",
    "\n",
    "levels = {'setosa':0, 'versicolor':1, 'virginica':2}\n",
    "Labels =  np.array([levels[x] for x in iris['Species']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 100 of the 150 cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 100)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. The code in the cell below performs the following processing:\n",
    "\n",
    "1. A Zscore scale object is defined using the `StandarScaler` function from the Scikit Learn preprocessing package. \n",
    "2. The scaler is fit to the training features. Subsequently, this scaler is used to apply the same scaling to the test data and in production. \n",
    "3. The training features are scaled using the `transform` method. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = preprocessing.StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a neural network model. The code in the cell below defines a single hidden layer neural network model with 50 units. The code uses the MLPClassifer function from the Scikit Lean neural_network package. The model is then fit. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "nn_mod = MLPClassifier(hidden_layer_sizes = (50,))\n",
    "nn_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the many neural network model object hyperparameters are displayed. Optimizing these parameters for a given situation can be quite time consuming. \n",
    "\n",
    "Next, the code in the cell below performs the following processing to score the test data subset:\n",
    "1. The test features are scaled using the scaler computed for the training features. \n",
    "2. The `predict` method is used to compute the scores from the scaled features. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scale.transform(X_test)\n",
    "scores = nn_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made difficult deliberately, by having more test cases than training cases. \n",
    "\n",
    "The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from pervious labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_metrics_3(labels, scores):\n",
    "   \n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score Setosa   Score Versicolor    Score Virginica')\n",
    "    print('Actual Setosa      %6d' % conf[0,0] + '            %5d' % conf[0,1] + '             %5d' % conf[0,2])\n",
    "    print('Actual Versicolor  %6d' % conf[1,0] + '            %5d' % conf[1,1] + '             %5d' % conf[1,2])\n",
    "    print('Actual Vriginica   %6d' % conf[2,0] + '            %5d' % conf[2,1] + '             %5d' % conf[2,2])\n",
    "    ## Now compute and display the accuracy and metrics\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    print(' ')\n",
    "    print('          Setosa  Versicolor  Virginica')\n",
    "    print('Num case   %0.2f' % metrics[3][0] + '     %0.2f' % metrics[3][1] + '      %0.2f' % metrics[3][2])\n",
    "    print('Precision   %0.2f' % metrics[0][0] + '      %0.2f' % metrics[0][1] + '       %0.2f' % metrics[0][2])\n",
    "    print('Recall      %0.2f' % metrics[1][0] + '      %0.2f' % metrics[1][1] + '       %0.2f' % metrics[1][2])\n",
    "    print('F1          %0.2f' % metrics[2][0] + '      %0.2f' % metrics[2][1] + '       %0.2f' % metrics[2][2])\n",
    "    \n",
    "print_metrics_3(y_test, scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified. \n",
    "2. The overall accuracy is 0.88. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n",
    "3. The precision, recall and  F1 for each of the classes is relatively good. Versicolor has the worst metrics since it has the largest number of misclassified cases. \n",
    "\n",
    "To get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_iris_score(iris, y_test, scores):\n",
    "    '''Function to plot iris data by type'''\n",
    "    ## Find correctly and incorrectly classified cases\n",
    "    true = np.equal(scores, y_test).astype(int)\n",
    "    \n",
    "    ## Create data frame from the test data\n",
    "    iris = pd.DataFrame(iris)\n",
    "    levels = {0:'setosa', 1:'versicolor', 2:'virginica'}\n",
    "    iris['Species'] = [levels[x] for x in y_test]\n",
    "    iris.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']\n",
    "    \n",
    "    ## Set up for the plot\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    markers = ['o', '+']\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    \n",
    "    for t in range(2): # loop over correct and incorect classifications\n",
    "        setosa = iris[(iris['Species'] == 'setosa') & (true == t)]\n",
    "        versicolor = iris[(iris['Species'] == 'versicolor') & (true == t)]\n",
    "        virginica = iris[(iris['Species'] == 'virginica') & (true == t)]\n",
    "        # loop over all the dimensions\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = markers[t], color = 'blue')\n",
    "                ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = markers[t], color = 'orange')\n",
    "                ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = markers[t], color = 'green')\n",
    "                ax[i,j].set_xlabel(x_ax[i])\n",
    "                ax[i,j].set_ylabel(y_ax[j])\n",
    "\n",
    "plot_iris_score(X_test, y_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected. There is an error in classifying Setosa which is a bit surprising, and which probably arises from the projection of the division between classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible that a more complex neural network would separate these cases better? The more complex model should have greater model capacity, but will be more susceptible to over-fitting. The code in the cell below uses a neural network with 2 hidden layers and 100 units per layer, coded as (100,100). This model is fit with the training data and displays the evaluation of the model. \n",
    "\n",
    "Execute this code, and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "nn_mod = MLPClassifier(hidden_layer_sizes = (100,100),\n",
    "                       max_iter=300)\n",
    "nn_mod.fit(X_train, y_train)\n",
    "scores = nn_mod.predict(X_test)\n",
    "print_metrics_3(y_test, scores) \n",
    "plot_iris_score(X_test, y_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are remarkably good results. Apparently, adding additional model capacity allowed the neural network model to perform exceptionally well. There are only 7 misclassified cases, giving an overall accuracy of 0.93.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which had the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "3. Encoding categorical variables as binary dummy variables.\n",
    "4. Standardizing numeric variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network training is known to be problematic when there is significant class imbalance. Unfortunately, neural networks have no method for weighting cases. Some alternatives are:\n",
    "1. **Impute** new values using a statistical algorithm. \n",
    "2. **Undersample** the majority cases. For this method a number of the cases equal to the minority case are Bernoulli sampled from the majority case. \n",
    "3. **Oversample** the minority cases. For this method the number of minority cases are resampled until they equal the number of majority cases.\n",
    "\n",
    "The code in the cell below oversamples the minority cases; bad credit customers. Execute this code to create a data set with balanced cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_Labels = Labels[Labels == 1] \n",
    "temp_Features = Features[Labels == 1,:]\n",
    "temp_Features = np.concatenate((Features, temp_Features), axis = 0)\n",
    "temp_Labels = np.concatenate((Labels, temp_Labels), axis = 0) \n",
    "\n",
    "print(temp_Features.shape)\n",
    "print(temp_Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross validation is used to estimate the optimal hyperparameters and perform model selection for a neural network model. 3 fold cross validation is used since training neural networks is computationally intensive. Additional folds would give better estimates but at the cost of greater computation time. Execute the code in the cell below to define inside and outside fold objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=3, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=3, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below estimates the best hyperparameters using 3 fold cross validation. In the interest of computational efficiency, values for only 4 parameters will be searched. There are several points to note here:\n",
    "1. In this case, a grid of four hyperparameters: \n",
    "  - **alpha** is the l2 regularization hyperparameter, \n",
    "  - **early_stopping** determines when the training metric becomes worse following an iteration of the optimization algorithm stops the training at the previous iteration. Early stopping is a powerful method to prevent over-fitting of machine learning models in general and neural networks in particular,\n",
    "  - **beta_1** and **beta_2** are hyperparameters that control the adaptive learning rate used by the **Adam** optimizer,\n",
    "3. The model is fit on the grid, and\n",
    "4. The best estimated hyperparameters are printed. \n",
    "\n",
    "This code searches over a 3X3X3X2 or 54 element grid using 3 fold cross validation. Using even this modest search grid and number of folds requires the model to be trained 162 times. Execute this code and examine the result, but expect execution to take some time. \n",
    "\n",
    "Once you have executed the code, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid = {#\"alpha\":[0.0000001,0.000001,0.00001], \n",
    "              #\"early_stopping\":[True, False], \n",
    "              \"beta_1\":[0.95,0.90,0.80], \n",
    "              \"beta_2\":[0.999,0.9,0.8]}\n",
    "\n",
    "## Define the Neural Network model\n",
    "nn_clf = MLPClassifier(hidden_layer_sizes = (100,100),\n",
    "                       max_iter=300)\n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "nr.seed(3456)\n",
    "nn_clf = ms.GridSearchCV(estimator = nn_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = 'recall',\n",
    "                      return_train_score = True)\n",
    "\n",
    "nr.seed(6677)\n",
    "nn_clf.fit(temp_Features, temp_Labels)\n",
    "#print(nn_clf.best_estimator_.alpha)\n",
    "#print(nn_clf.best_estimator_.early_stopping)\n",
    "print(nn_clf.best_estimator_.beta_1)\n",
    "print(nn_clf.best_estimator_.beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will run the code in the cell below to perform the outer cross validation of the model. The multiple trainings of this model will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(nn_clf, temp_Features, temp_Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of Recall is an order of magnitude less than the mean itself. This indicates that this model is likely to generalize well, but the level of performance is still unclear. \n",
    "\n",
    "Now, you will build and test a model using the estimated optimal hyperparameters. However, there is a complication. The training data subset must have the minority case oversampled. Execute the code in the cell below to create training and testing dataset, with oversampled minority cases for the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])\n",
    "\n",
    "## Oversample the minority case for the training data\n",
    "y_temp = y_train[y_train == 1] \n",
    "X_temp = X_train[y_train == 1,:]\n",
    "X_train = np.concatenate((X_train, X_temp), axis = 0)\n",
    "y_train = np.concatenate((y_train, y_temp), axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines a neural network model object using the estimated optimal model hyperparameters and then fits the model to the training data. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "nn_mod = MLPClassifier(hidden_layer_sizes = (100,100), \n",
    "                       #alpha = nn_clf.best_estimator_.alpha, \n",
    "                       #early_stopping = nn_clf.best_estimator_.early_stopping, \n",
    "                       beta_1 = nn_clf.best_estimator_.beta_1, \n",
    "                       beta_2 = nn_clf.best_estimator_.beta_2,\n",
    "                       max_iter = 300)\n",
    "nn_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the hyperparameters of the  neural network model object reflect those specified. \n",
    "\n",
    "The code in the cell below scores and prints evaluation metrics for the model, using the test data subset. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 3** on the course page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = nn_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the neural network model is less than ideal. For the negative (bad credit) case the recall is perhaps adequate, but the precision is poor. Perhaps the oversampling does not help much in this case. Challenge yourself - try and perform the undersampling method and compare the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used neural models to classify the cases of the iris data. The model with greater capacity achieved significantly better results. \n",
    "2. Used 3 fold to find estimated optimal hyperparameters for a neural network model to classify credit risk cases. Oversampling of the minority case for the training data was required to deal with the class imbalance. Despite this approach, the results achieved are marginal at best. Perhaps, a model with greater capacity would achieve better results, or a different approach to dealing with class imbalance would be more successful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
