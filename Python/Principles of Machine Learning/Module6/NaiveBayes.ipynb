{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Models\n",
    "\n",
    "In this lab you will work with **naive Bayes models**. Naive Bayes models are a surprisingly useful and effective simplification of the general Bayesian models. Naive Bayes models make the naive assumption of statistical independence of the features. In many cases, naive Bayes module are surprisingly effective despite violating the assumption of independence. \n",
    "\n",
    "In simple terms, naive Bayes models use empirical distributions of the features to compute probabilities of the labels. The naive Bayes models can use most any family of distributions for the features. It is important to select the correct distribution family for the data you are working with. Common cases are:\n",
    "- **Gaussian;** for continuous or numerical features.\n",
    "- **Bernoulli;** for features with binary values. \n",
    "- **Multinomial;** for features with more than two categories. \n",
    "\n",
    "These is one pit fall, the model fails if a zero probability is encountered. This situation occurs when there is a 'hole' in the sample space where there are no samples. A simple smoothing procedure can deal with this problem. The smoothing hyperparameter, usually called alpha, is one of the few required for naive Bayes models. \n",
    "\n",
    "Some properties of naive Bayes models are:\n",
    "- Computational complexity is linear in number of parameter/features, making naive Bayes models highly scalable. There are out or core approaches suitable for massive datasets.\n",
    "- Requires minimal data to produce models that generalizes well. If there are only a few cases per category to train a model a naive Bayes model can be a good choice. \n",
    "- Have a simple and inherent regularization.\n",
    "\n",
    "Naive Bayes models are used in many situations including:\n",
    "\n",
    "- Document classification\n",
    "- SPAM detection\n",
    "- Image classification \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use a naive Bayes model to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for these data, you will now load and plot them. The code in the cell below does the following:\n",
    "\n",
    "1. Loads the iris data as a Pandas data frame. \n",
    "2. Adds column names to the data frame.\n",
    "3. Displays all 4 possible scatter plot views of the data. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris(iris):\n",
    "    '''Function to plot iris data by type'''\n",
    "    setosa = iris[iris['Species'] == 'setosa']\n",
    "    versicolor = iris[iris['Species'] == 'versicolor']\n",
    "    virginica = iris[iris['Species'] == 'virginica']\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = 'x')\n",
    "            ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = 'o')\n",
    "            ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = '+')\n",
    "            ax[i,j].set_xlabel(x_ax[i])\n",
    "            ax[i,j].set_ylabel(y_ax[j])\n",
    "            \n",
    "## Import the dataset from sklearn.datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "## Create a data frame from the dictionary\n",
    "species = [iris.target_names[x] for x in iris.target]\n",
    "iris = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\n",
    "iris['Species'] = species\n",
    "\n",
    "## Plot views of the iris data            \n",
    "plot_iris(iris) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (blue) is well separated from the other two categories. The Versicolor (orange) and the Virginica (green) show considerable overlap. The question is how well our classifier will separate these categories. \n",
    "\n",
    "Scikit Learn classifiers require numerically coded numpy arrays for the features and as a label. The code in the cell below does the following processing:\n",
    "1. Creates a numpy array of the features.\n",
    "2. Numerically codes the label using a dictionary lookup, and converts it to a numpy array. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(iris[['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']])\n",
    "\n",
    "levels = {'setosa':0, 'versicolor':1, 'virginica':2}\n",
    "Labels =  np.array([levels[x] for x in iris['Species']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 100 of the 150 cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 100)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. The code in the cell below performs the following processing:\n",
    "\n",
    "1. A Zscore scale object is defined using the `StandarScaler` function from the scikit-learn preprocessing package. \n",
    "2. The scaler is fit to the training features. Subsequently, this scaler is used to apply the same scaling to the test data and in production. \n",
    "3. The training features are scaled using the `transform` method. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = preprocessing.StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a Gaussian naive Bayes model. A Gaussian model is appropriate here since all of the features are numeric. \n",
    "\n",
    "The code in the cell below defines a Gaussian naive Bayes model object using the `GaussianNB` function from the scikit-learn naive_bayes package, and then fits the model. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB_mod = GaussianNB()\n",
    "NB_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Gaussian naive Bayes model object has only one hyperparameter. \n",
    "\n",
    "Next, the code in the cell below performs the following processing to score the test data subset:\n",
    "1. The test features are scaled using the scaler computed for the training features. \n",
    "2. The `predict` method is used to compute the scores from the scaled features. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scale.transform(X_test)\n",
    "scores = NB_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made deliberately difficult, by having more test cases than training cases. \n",
    "\n",
    "The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from previous labs to deal with a three category problem. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_metrics_3(labels, scores):\n",
    "   \n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score Setosa   Score Versicolor    Score Virginica')\n",
    "    print('Actual Setosa      %6d' % conf[0,0] + '            %5d' % conf[0,1] + '             %5d' % conf[0,2])\n",
    "    print('Actual Versicolor  %6d' % conf[1,0] + '            %5d' % conf[1,1] + '             %5d' % conf[1,2])\n",
    "    print('Actual Vriginica   %6d' % conf[2,0] + '            %5d' % conf[2,1] + '             %5d' % conf[2,2])\n",
    "    ## Now compute and display the accuracy and metrics\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    print(' ')\n",
    "    print('          Setosa  Versicolor  Virginica')\n",
    "    print('Num case   %0.2f' % metrics[3][0] + '     %0.2f' % metrics[3][1] + '      %0.2f' % metrics[3][2])\n",
    "    print('Precision   %0.2f' % metrics[0][0] + '      %0.2f' % metrics[0][1] + '       %0.2f' % metrics[0][2])\n",
    "    print('Recall      %0.2f' % metrics[1][0] + '      %0.2f' % metrics[1][1] + '       %0.2f' % metrics[1][2])\n",
    "    print('F1          %0.2f' % metrics[2][0] + '      %0.2f' % metrics[2][1] + '       %0.2f' % metrics[2][2])\n",
    "    \n",
    "print_metrics_3(y_test, scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified. \n",
    "2. The overall accuracy is 0.91. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. As was mentioned previously, naive Bayes models require only small amounts of training data. \n",
    "3. The precision, recall and  F1 for each of the classes is relatively good. Versicolor has the worst metrics since it has the largest number of misclassified cases. \n",
    "\n",
    "To get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_iris_score(iris, y_test, scores):\n",
    "    '''Function to plot iris data by type'''\n",
    "    ## Find correctly and incorrectly classified cases\n",
    "    true = np.equal(scores, y_test).astype(int)\n",
    "    \n",
    "    ## Create data frame from the test data\n",
    "    iris = pd.DataFrame(iris)\n",
    "    levels = {0:'setosa', 1:'versicolor', 2:'virginica'}\n",
    "    iris['Species'] = [levels[x] for x in y_test]\n",
    "    iris.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']\n",
    "    \n",
    "    ## Set up for the plot\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    markers = ['o', '+']\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    \n",
    "    for t in range(2): # loop over correct and incorect classifications\n",
    "        setosa = iris[(iris['Species'] == 'setosa') & (true == t)]\n",
    "        versicolor = iris[(iris['Species'] == 'versicolor') & (true == t)]\n",
    "        virginica = iris[(iris['Species'] == 'virginica') & (true == t)]\n",
    "        # loop over all the dimensions\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = markers[t], color = 'blue')\n",
    "                ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = markers[t], color = 'orange')\n",
    "                ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = markers[t], color = 'green')\n",
    "                ax[i,j].set_xlabel(x_ax[i])\n",
    "                ax[i,j].set_ylabel(y_ax[j])\n",
    "\n",
    "plot_iris_score(X_test, y_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which had the the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "3. Encoding categorical variables as binary dummy variables.\n",
    "4. Standardizing numeric variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Features array has both numeric features and binary features (dummy variables for the categorical features). Therefore, a Gaussian model must be used. However, this model is not ideal, since numeric features are mixed with features exhibiting Bernoulli distributions, the binary features. \n",
    "\n",
    "The code in the cell below does the following processing:\n",
    "1. Defines a 10 fold cross validation object. \n",
    "2. Defines a Gaussian naive Bayes model.\n",
    "3. Performs a 10 fold cross validation.\n",
    "4. Prints results from the cross validation. \n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(321)\n",
    "cv_folds = ms.KFold(n_splits=10, shuffle = True)\n",
    "    \n",
    "nr.seed(498)\n",
    "NB_credit = GaussianNB()\n",
    "cv_estimate = ms.cross_val_score(NB_credit, Features, Labels, \n",
    "                                 cv = cv_folds) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(498)\n",
    "NB_credit = GaussianNB()\n",
    "cv_estimate = ms.cross_val_score(NB_credit, Features, Labels, \n",
    "                                 cv = 10) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of the AUC is more than an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "Now, you will build and test a model using a single split of the dataset. As a first step, execute the code in the cell below to create training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines a naive Bayes model object and then fits the model to the training data. Execute this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_credit_mod = GaussianNB() \n",
    "NB_credit_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the test data subset, the code in the cell below scores and prints evaluation metrics for the model. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 2** on the course page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these performance metrics are poor. Barely half the bad credit risk customers are correctly identified. The reported AUC is quite a bit better than the mean achieved with the 5 fold cross validation. It is likely these figures are optimistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, a Bernoulli naive Bayes model will work better. Since naive Bayes models tend to be less sensitive to the quantity of training data, this approach may be reasonable. To apply this model, the numeric features must be dropped from the array. Execute the code in the cell below to remove the numeric features and examine the resulting sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = Features[:,4:]\n",
    "Features[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all of the features are binary. A Bernoulli naive Bayes can now be applied to these features. \n",
    "\n",
    "10 fold nested cross validation is used to estimate the optimal hyperparameter and performs model selection for the naive Bayes model. Execute the code in the cell below to define inside and outside fold objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=10, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below estimates the best hyperparameters using 10 fold cross validation. There are two points to notice here:\n",
    "1. In this case, a grid for one hyperparameter: alpha is the smoothing parameter to avoid zero probabilities.  \n",
    "3. The model is fit on the grid and the best estimated hyperparameters are printed. \n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(3456)\n",
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "## Define the NB regression model\n",
    "NB_clf = BernoulliNB() \n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "clf = ms.GridSearchCV(estimator = NB_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = 'roc_auc',\n",
    "                      return_train_score = True)\n",
    "clf.fit(Features, Labels)\n",
    "print(clf.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest alpha hyperparameter has been selected. This indicates that there is very little problem with zero probabilities in this problem. This situation results from the fact that the probability space sampled is dense. \n",
    "\n",
    "The code in the cell below executes the outer loop of the cross validation to estimate model performance with the optimal hyperparameter. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB_credit = BernoulliNB(alpha = clf.best_estimator_.alpha)\n",
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(clf, Features, Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first look, the AUC seems reasonable. The standard deviation is an order of magnitude less than the AUC. \n",
    "\n",
    "Next, sample the dataset into training and testing subsets by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code below to fit and score the Bernoulli naive Bayes model and display the performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_credit_mod = BernoulliNB(alpha = clf.best_estimator_.alpha) \n",
    "NB_credit_mod.fit(X_train, y_train)\n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this Bernoulli naive Bayes model are much better than for the Gaussian model. Still, they could be better.\n",
    "\n",
    "The current model uses the empirical distribution of the label values for the prior value of $p$ of the Bernoulli distribution. This probability is invariably skewed toward the majority case. Since the bank cares more about the minority case, setting this distribution to a fixed prior value can help overcome the class imbalance. The code in the cell below redefines the model object with prior probability of 0.6 for the minority case. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB_credit_mod = BernoulliNB(alpha = clf.best_estimator_.alpha,\n",
    "                            class_prior = [0.4,0.6]) \n",
    "NB_credit_mod.fit(X_train, y_train)\n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of bad credit cases are now correctly identified. However, this characteristic is at the cost of a high false negative error rate. Still, given that the cost to the bank of a false negative is five times the cost of a false positive, this may be a good solution. An infinite number of other models are possible by changing the prior distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used a Gaussian naive model to classify the cases of the iris data. The overall model performance was reasonable. \n",
    "2. Fit a Gaussian naive Bayes model on the bank credit data. The performance of this model was poor as a result of many Bernoulli distributed dummy features.\n",
    "3. Used a Bernoulli naive Bayes model for the bank credit data by eliminating the numeric features. Overall, this model was much better. \n",
    "4. A model skewed toward detecting bad credit cases was created using a prior distribution rather than the empirical distribution of the labels. This model correctly classified a significant number of the bad credit cases. Selecting other prior distributions will give other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
