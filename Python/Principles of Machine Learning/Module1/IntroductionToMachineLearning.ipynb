{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "This lab introduces some basic concepts of machine learning with Python. In this lab you will use the K-Nearest Neighbor (KNN) algorithm to classify the species of iris flowers, given measurements of flower characteristics. By completing this lab you will have an overview of an end-to-end machine learning modeling process.  \n",
    "\n",
    "By the completion of this lab, you will:\n",
    "1. Follow and understand a complete end-to-end machine learning process including data exploration, data preparation, modeling, and model evaluation. \n",
    "2. Develop a basic understanding of the principles of machine learning and associated terminology. \n",
    "3. Understand the basic process for evaluating machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of KNN classification\n",
    "\n",
    "Before discussing a specific algorithm, it helps to know a bit of machine learning terminology. In supervised machine learning a set of ***cases*** are used to ***train***, ***test*** and ***evaluate*** the model. Each case is comprised of the values of one or more ***features*** and a ***label*** value. The features are variables used by the model to ***predict** the value of the label. Minimizing the ***errors*** between the true value of the label and the prediction supervises the training of this model. Once the model is trained and tested, it can be evaluated based on the accuracy in predicting the label of a new set of cases. \n",
    "\n",
    "In this lab you will use randomly selected cases to first train and then evaluate a k-nearest-neighbor (KNN) machine learning model. The goal is to predict the type or class of the label, which makes the machine learning model a ***classification*** model. \n",
    "\n",
    "The k-nearest-neighbor algorithm is conceptually simple. In fact, there is no formal training step. Given a known set of cases, a new case is classified by majority vote of the K (where $k = 1, 2, 3$, etc.) points nearest to the values of the new case; that is, the nearest neighbors of the new case. \n",
    "\n",
    "The schematic figure below illustrates the basic concepts of a KNN classifier. In this case there are two features, the values of one shown on the horizontal axis and the values of the other shown on the vertical axis. The cases are shown on the diagram as one of two classes, red triangles and blue circles. To summarize, each case has a value for the two features, and a class. The goal of the KNN algorithm is to classify cases with unknown labels. \n",
    "\n",
    "Continuing with the example, on the left side of the diagram the $K = 1$ case is illustrated. The nearest neighbor is a red triangle. Therefore, this KNN algorithm will classify the unknown case, '?', as a red triangle. On the right side of the diagram, the $K = 3$ case is illustrated. There are three near neighbors within the circle. The majority of nearest neighbors for $K = 3$ are the blue circles, so the algorithm classifies the unknown case, '?', as a blue circle. Notice that class predicted for the unknown case changes as K changes. This behavior is inherent in the KNN method.  \n",
    "\n",
    "![](img/KNN.jpg)\n",
    "<center> **KNN for k = 1 and k = 3**</center>\n",
    "\n",
    "There are some additional considerations in creating a robust KNN algorithm. These will be addressed later in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data set\n",
    "\n",
    "In this lab you will work with the Iris data set. This data set is famous in the history of statistics. The first publication using these data in statistics by the pioneering statistician Ronald A Fisher was in 1936. Fisher proposed an algorithm to classify the species of iris flowers from physical measurements of their characteristics. The data set has been used as a teaching example ever since. \n",
    "\n",
    "Now, you will load and examine these data which are in the statsmodels.api package. Execute the code in the cell below and examine the first few rows of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "\n",
    "## Import the dataset from sklearn.datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "## Create a data frame from the dictionary\n",
    "species = [iris.target_names[x] for x in iris.target]\n",
    "iris = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\n",
    "iris['Species'] = species\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four features, containing the dimensions of parts of the iris flower structures. The label column is the Species of the flower. The goal is to create and test a KNN algorithm to correctly classify the species. \n",
    "\n",
    "Next, you will execute the code in the cell below to show the data types of each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are all numeric, and the label is a categorical string variable.\n",
    "\n",
    "Next, you will determine the number of unique categories, and number of cases for each category, for the label variable, Species. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['count'] = 1\n",
    "iris[['Species', 'count']].groupby('Species').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are three species of iris, each with 50 cases. \n",
    "\n",
    "Next, you will create some plots to see how the classes might, or might not, be well separated by the value of the features. In an ideal case, the label classes will be perfectly separated by one or more of the feature pairs. In the real-world this ideal situation will rarely, if ever, be the case.\n",
    " \n",
    "There are six possible pair-wise scatter plots of these four features. For now, we will just create scatter plots of two variable pairs. Execute the code in the cell below and examine the resulting plots.\n",
    "***\n",
    "**Note:** Data visualization and the Seaborn package are covered in another lesson.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_iris(iris, col1, col2):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    sns.lmplot(x = col1, y = col2, \n",
    "               data = iris, \n",
    "               hue = \"Species\", \n",
    "               fit_reg = False)\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.title('Iris species shown by color')\n",
    "    plt.show()\n",
    "plot_iris(iris, 'Petal_Width', 'Sepal_Length')\n",
    "plot_iris(iris, 'Sepal_Width', 'Sepal_Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results noticing the separation, or overlap, of the label values. \n",
    "\n",
    "Then, answer **Question 1** on the course page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data set\n",
    "\n",
    "Data preparation is an important step before training any machine learning model. These data require only two preparation steps:\n",
    "- Scale the numeric values of the features. It is important that numeric features used to train machine learning models have a similar range of values. Otherwise, features which happen to have large numeric values may dominate model training, even if other features with smaller numeric values are more informative. In this case Zscore normalization is used. This normalization process scales each feature so that the mean is 0 and the variance is 1.0. \n",
    "- Split the dataset into randomly sampled training and evaluation data sets. The random selection of cases seeks to limit the leakage of information between the training and evaluation cases.\n",
    "\n",
    "The code in the cell below normalizes the features by these steps:\n",
    "- The scale function from scikit-learn.preprocessing is used to normalize the features.\n",
    "- Column names are assigned to the resulting data frame. \n",
    "- A statitical summary of the data frame is then printed. \n",
    "\n",
    "***\n",
    "**Note:** Data preparation with scikit-learn is covered in another lesson. \n",
    "***\n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "num_cols = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n",
    "iris_scaled = scale(iris[num_cols])\n",
    "iris_scaled = pd.DataFrame(iris_scaled, columns = num_cols)\n",
    "print(iris_scaled.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. The mean of each column is zero and the standard deviation is approximately 1.0.\n",
    "\n",
    "The methods in the scikit-learn package requires numeric numpy arrays as arguments. Therefore, the strings indicting species must be re-coded as numbers. The code in the cell below does this using a dictionary lookup. Execute this code and examine the head of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {'setosa':0, 'versicolor':1, 'virginica':2}\n",
    "iris_scaled['Species'] = [levels[x] for x in iris['Species']]\n",
    "iris_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will split the dataset into a test and evaluation sub-sets. The code in the cell below randomly splits the dataset into training and testing subsets. The features and labels are then separated into numpy arrays. The dimension of each array is printed as a check. Execute this code to create these subsets. \n",
    "\n",
    "***\n",
    "**Note:** Splitting data sets for machine learning with scikit-learn is discussed in another lesson.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into a training and test set by Bernoulli sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(3456)\n",
    "iris_split = train_test_split(np.asmatrix(iris_scaled), test_size = 75)\n",
    "iris_train_features = iris_split[0][:, :4]\n",
    "iris_train_labels = np.ravel(iris_split[0][:, 4])\n",
    "iris_test_features = iris_split[1][:, :4]\n",
    "iris_test_labels = np.ravel(iris_split[1][:, 4])\n",
    "print(iris_train_features.shape)\n",
    "print(iris_train_labels.shape)\n",
    "print(iris_test_features.shape)\n",
    "print(iris_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the KNN model\n",
    "\n",
    "With some understanding of the relationships between the features and the label and preparation of the data completed you will now train and evaluate a $K = 3$ model. The code in the cell below does the following:\n",
    "- The KNN model is defined as having $K = 3$.\n",
    "- The model is trained using the fit method with the feature and label numpy arrays as arguments.\n",
    "- Displays a summary of the model. \n",
    "\n",
    "Execute this code and examine the summary of these results.\n",
    "\n",
    "\n",
    "***\n",
    "**Note:** Constructing machine learning models with scikit-learn is covered in another lesson.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define and train the KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_mod = KNeighborsClassifier(n_neighbors = 3)\n",
    "KNN_mod.fit(iris_train_features, iris_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will evaluate this model using the accuracy statistic and a set of plots. The following steps create model predictions and compute accuracy:\n",
    "- The predict method is used to compute KNN predictions from the model using the test features as an argument. \n",
    "- The predictions are scored as correct or not using a list comprehension. \n",
    "- Accuracy is computed as the percentage of the test cases correctly classified. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_test = pd.DataFrame(iris_test_features, columns = num_cols)\n",
    "iris_test['predicted'] = KNN_mod.predict(iris_test_features)\n",
    "iris_test['correct'] = [1 if x == z else 0 for x, z in zip(iris_test['predicted'], iris_test_labels)]\n",
    "accuracy = 100.0 * float(sum(iris_test['correct'])) / float(iris_test.shape[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is pretty good.\n",
    "\n",
    "Now, execute the code in the cell below and examine plots of the classifications of the iris species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {0:'setosa', 1:'versicolor', 2:'virginica'}\n",
    "iris_test['Species'] = [levels[x] for x in iris_test['predicted']]\n",
    "markers = {1:'^', 0:'o'}\n",
    "colors = {'setosa':'blue', 'versicolor':'green', 'virginica':'red'}\n",
    "def plot_shapes(df, col1,col2,  markers, colors):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    ax = plt.figure(figsize=(6, 6)).gca() # define plot axis\n",
    "    for m in markers: # iterate over marker dictioary keys\n",
    "        for c in colors: # iterate over color dictionary keys\n",
    "            df_temp = df[(df['correct'] == m)  & (df['Species'] == c)]\n",
    "            sns.regplot(x = col1, y = col2, \n",
    "                        data = df_temp,  \n",
    "                        fit_reg = False, \n",
    "                        scatter_kws={'color': colors[c]},\n",
    "                        marker = markers[m],\n",
    "                        ax = ax)\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.title('Iris species by color')\n",
    "    return 'Done'\n",
    "plot_shapes(iris_test, 'Petal_Width', 'Sepal_Length', markers, colors)\n",
    "plot_shapes(iris_test, 'Sepal_Width', 'Sepal_Length', markers, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the plots above color is used to show the predicted class. Correctly classified cases are shown by triangles and incorrectly classified cases are shown by circles. \n",
    "\n",
    "Examine the plot and answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have created and evaluated a KNN machine learning classification model. Specifically you have:\n",
    "1. Loaded and explored the data using visualization to determine if the features separate the classes.\n",
    "2. Prepared the data by normalizing the numeric features and randomly sampling into training and testing subsets. \n",
    "3. Constructing and evaluating the machine learning model. Evaluation was performed by statistically, with the accuracy metric, and with visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
